{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78555c5-7cce-4405-9d8f-e84c83e93562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import streamlit as st\n",
    "from streamlit_option_menu import option_menu\n",
    "from streamlit_extras.add_vertical_space import add_vertical_space\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def streamlit_config():\n",
    "    # page configuration\n",
    "    st.set_page_config(page_title='Smart Resume Parser', layout=\"wide\")\n",
    "\n",
    "    page_background_color = \"\"\"\n",
    "    <style>\n",
    "    body {\n",
    "        background-color: #20B2AA;\n",
    "    }\n",
    "    [data-testid=\"stHeader\"] {\n",
    "        background: rgba(0,0,0,0);\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "    \n",
    "    st.markdown(page_background_color, unsafe_allow_html=True)\n",
    "\n",
    "    # title and position\n",
    "    st.markdown(f'<h1 style=\"text-align: center;\">Smart Resume Parser</h1>',\n",
    "                unsafe_allow_html=True)\n",
    "\n",
    "    # Display the image from URL\n",
    "    st.image(\"https://www.recruitbpm.com/blog/wp-content/uploads/2023/04/Resume-Parser-Software.png\", caption=\"\", use_container_width=True)\n",
    "    \n",
    "class resume_analyzer:\n",
    "\n",
    "    @staticmethod\n",
    "    def pdf_to_chunks(pdf):\n",
    "        # Read the PDF and return text chunks\n",
    "        pdf_reader = PdfReader(pdf)\n",
    "        text = \"\"\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "        # Split the long text into small chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200, length_function=len)\n",
    "        chunks = text_splitter.split_text(text=text)\n",
    "        return chunks\n",
    "\n",
    "    @staticmethod\n",
    "    def openai(openai_api_key, chunks, analyze):\n",
    "        # OpenAI service for embedding\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "        vectorstores = FAISS.from_texts(chunks, embedding=embeddings)\n",
    "\n",
    "        # Search for the most similar chunks based on the query\n",
    "        docs = vectorstores.similarity_search(query=analyze, k=3)\n",
    "\n",
    "        # Create an OpenAI object, using ChatGPT 3.5 Turbo model\n",
    "        llm = ChatOpenAI(model='gpt-3.5-turbo', api_key=openai_api_key)\n",
    "        chain = load_qa_chain(llm=llm, chain_type='stuff')\n",
    "\n",
    "        # Run the question-answering pipeline\n",
    "        response = chain.run(input_documents=docs, question=analyze)\n",
    "        return response\n",
    "\n",
    "    @staticmethod\n",
    "    def summary_prompt(query_with_chunks):\n",
    "        return f''' need to detailed summarization of below resume and finally conclude them\n",
    "\n",
    "                    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                    {query_with_chunks}\n",
    "                    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                    '''\n",
    "    @staticmethod\n",
    "    def summarize_skills_query(summary):\n",
    "        return f'''Identify and list the key skills mentioned in the following resume summary:\n",
    "\n",
    "                \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                {summary}\n",
    "                \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "            '''\n",
    "    @staticmethod  \n",
    "    def recommend_skills_query(summary):\n",
    "        return f\"Based on the resume, identify skills the candidate possesses and recommend additional skills for career growth:\\n{summary}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def recommend_courses_query(summary):\n",
    "        return f\"Suggest and courses or certifications to enhance the skills mentioned in the resume and improve the candidate's career prospects and list them:\\n{summary}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def resume_tips_query(summary):\n",
    "        return f\"List professional tips to improve this resume for better presentation and relevance in simple points:\\n{summary}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def resume_summary(pdf, openai_api_key):\n",
    "        pdf_chunks = resume_analyzer.pdf_to_chunks(pdf)\n",
    "        summary_prompt = resume_analyzer.summary_prompt(query_with_chunks=pdf_chunks)\n",
    "        summary = resume_analyzer.openai(openai_api_key=openai_api_key, chunks=pdf_chunks, analyze=summary_prompt)\n",
    "        return summary\n",
    "\n",
    "    @staticmethod\n",
    "    def resume_strength(pdf, openai_api_key):\n",
    "        pdf_chunks = resume_analyzer.pdf_to_chunks(pdf)\n",
    "        summary = resume_analyzer.resume_summary(pdf, openai_api_key)\n",
    "        strength_prompt = f'''need to detailed analysis and explain of the strength of below resume and finally conclude them\n",
    "\n",
    "                    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                    {summary}\n",
    "                    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                    '''\n",
    "        strength = resume_analyzer.openai(openai_api_key=openai_api_key, chunks=pdf_chunks, analyze=strength_prompt)\n",
    "        return strength\n",
    "\n",
    "    @staticmethod\n",
    "    def resume_weakness(pdf, openai_api_key):\n",
    "        pdf_chunks = resume_analyzer.pdf_to_chunks(pdf)\n",
    "        summary = resume_analyzer.resume_summary(pdf, openai_api_key)\n",
    "        weakness_prompt = f'''need to detailed analysis and explain of the weakness of below resume and how to improve make a better resume.\n",
    "\n",
    "                    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                    {summary}\n",
    "                    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                    '''\n",
    "        weakness = resume_analyzer.openai(openai_api_key=openai_api_key, chunks=pdf_chunks, analyze=weakness_prompt)\n",
    "        return weakness\n",
    "\n",
    "    @staticmethod\n",
    "    def job_title_suggestion(pdf, openai_api_key):\n",
    "        pdf_chunks = resume_analyzer.pdf_to_chunks(pdf)\n",
    "        summary = resume_analyzer.resume_summary(pdf, openai_api_key)\n",
    "        job_title_prompt = f'''what are the job roles i can apply to based on the below resume?\n",
    "\n",
    "                    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                    {summary}\n",
    "                    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "                    '''\n",
    "        job_title = resume_analyzer.openai(openai_api_key=openai_api_key, chunks=pdf_chunks, analyze=job_title_prompt)\n",
    "        return job_title\n",
    "\n",
    "    @staticmethod\n",
    "    def skills_and_courses(pdf, openai_api_key):\n",
    "        summary = resume_analyzer.resume_summary(pdf, openai_api_key)\n",
    "        skills_query = resume_analyzer.summarize_skills_query(summary)\n",
    "        skills_result = resume_analyzer.openai(openai_api_key=openai_api_key, chunks=[summary], analyze=skills_query)\n",
    "\n",
    "        recommended_skills_query = resume_analyzer.recommend_skills_query(summary)\n",
    "        recommended_skills_result = resume_analyzer.openai(openai_api_key=openai_api_key, chunks=[summary], analyze=recommended_skills_query)\n",
    "        \n",
    "        courses_query = resume_analyzer.recommend_courses_query(summary)\n",
    "        courses = resume_analyzer.openai(openai_api_key=openai_api_key, chunks=[summary], analyze=courses_query)\n",
    "        \n",
    "        return skills_result, recommended_skills_result, courses\n",
    "\n",
    "           \n",
    "\n",
    "    @staticmethod\n",
    "    def resume_tips(pdf, openai_api_key):\n",
    "        summary = resume_analyzer.resume_summary(pdf, openai_api_key)\n",
    "        tips_query = resume_analyzer.resume_tips_query(summary)\n",
    "        tips = resume_analyzer.openai(openai_api_key=openai_api_key, chunks=[summary], analyze=tips_query)\n",
    "        return tips\n",
    "\n",
    "\n",
    "class linkedin_scraper:\n",
    "\n",
    "    def webdriver_setup():\n",
    "            \n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.maximize_window()\n",
    "        return driver\n",
    "\n",
    "\n",
    "    def get_userinput():\n",
    "\n",
    "        add_vertical_space(2)\n",
    "        with st.form(key='linkedin_scarp'):\n",
    "\n",
    "            add_vertical_space(1)\n",
    "            col1,col2,col3 = st.columns([0.5,0.3,0.2], gap='medium')\n",
    "            with col1:\n",
    "                job_title_input = st.text_input(label='Job Title')\n",
    "                job_title_input = job_title_input.split(',')\n",
    "            with col2:\n",
    "                job_location = st.text_input(label='Job Location', value='United States')\n",
    "            with col3:\n",
    "                job_count = st.number_input(label='Job Count', min_value=1, value=1, step=1)\n",
    "\n",
    "            # Submit Button\n",
    "            add_vertical_space(1)\n",
    "            submit = st.form_submit_button(label='Submit')\n",
    "            add_vertical_space(1)\n",
    "        \n",
    "        return job_title_input, job_location, job_count, submit\n",
    "\n",
    "\n",
    "    def build_url(job_title, job_location):\n",
    "\n",
    "        b = []\n",
    "        for i in job_title:\n",
    "            x = i.split()\n",
    "            y = '%20'.join(x)\n",
    "            b.append(y)\n",
    "\n",
    "        job_title = '%2C%20'.join(b)\n",
    "        link = f\"https://www.linkedin.com/jobs/search/?currentJobId=4061304622&geoId=103644278&keywords={job_title}&origin=JOB_SEARCH_PAGE_SEARCH_BUTTON&refresh=true\"\n",
    "\n",
    "        return link\n",
    "    \n",
    "\n",
    "    def open_link(driver, link):\n",
    "\n",
    "        while True:\n",
    "            # Break the Loop if the Element is Found, Indicating the Page Loaded Correctly\n",
    "            try:\n",
    "                driver.get(link)\n",
    "                driver.implicitly_wait(5)\n",
    "                time.sleep(3)\n",
    "                driver.find_element(by=By.CSS_SELECTOR, value='span.switcher-tabs__placeholder-text.m-auto')\n",
    "                return\n",
    "            \n",
    "            # Retry Loading the Page\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "\n",
    "\n",
    "    def link_open_scrolldown(driver, link, job_count):\n",
    "        \n",
    "        # Open the Link in LinkedIn\n",
    "        linkedin_scraper.open_link(driver, link)\n",
    "\n",
    "        # Scroll Down the Page\n",
    "        for i in range(0,job_count):\n",
    "\n",
    "            # Simulate clicking the Page Up button\n",
    "            body = driver.find_element(by=By.TAG_NAME, value='body')\n",
    "            body.send_keys(Keys.PAGE_UP)\n",
    "\n",
    "            # Locate the sign-in modal dialog \n",
    "            try:\n",
    "                driver.find_element(by=By.CSS_SELECTOR, \n",
    "                                value=\"button[data-tracking-control-name='public_jobs_contextual-sign-in-modal_modal_dismiss']>icon>svg\").click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Scoll down the Page to End\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            driver.implicitly_wait(2)\n",
    "\n",
    "            # Click on See More Jobs Button if Present\n",
    "            try:\n",
    "                x = driver.find_element(by=By.CSS_SELECTOR, value=\"button[aria-label='See more jobs']\").click()\n",
    "                driver.implicitly_wait(5)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "    def job_title_filter(scrap_job_title, user_job_title_input):\n",
    "        \n",
    "        # User Job Title Convert into Lower Case\n",
    "        user_input = [i.lower().strip() for i in user_job_title_input]\n",
    "\n",
    "        # scraped Job Title Convert into Lower Case\n",
    "        scrap_title = [i.lower().strip() for i in [scrap_job_title]]\n",
    "\n",
    "        # Verify Any User Job Title in the scraped Job Title\n",
    "        confirmation_count = 0\n",
    "        for i in user_input:\n",
    "            if all(j in scrap_title[0] for j in i.split()):\n",
    "                confirmation_count += 1\n",
    "\n",
    "        # Return Job Title if confirmation_count greater than 0 else return NaN\n",
    "        if confirmation_count > 0:\n",
    "            return scrap_job_title\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "    def scrap_company_data(driver, job_title_input, job_location):\n",
    "\n",
    "        # scraping the Company Data\n",
    "        company = driver.find_elements(by=By.CSS_SELECTOR, value='h4[class=\"base-search-card__subtitle\"]')\n",
    "        company_name = [i.text for i in company]\n",
    "\n",
    "        location = driver.find_elements(by=By.CSS_SELECTOR, value='span[class=\"job-search-card__location\"]')\n",
    "        company_location = [i.text for i in location]\n",
    "\n",
    "        title = driver.find_elements(by=By.CSS_SELECTOR, value='h3[class=\"base-search-card__title\"]')\n",
    "        job_title = [i.text for i in title]\n",
    "\n",
    "        url = driver.find_elements(by=By.XPATH, value='//a[contains(@href, \"/jobs/\")]')\n",
    "        website_url = [i.get_attribute('href') for i in url]\n",
    "\n",
    "        # combine the all data to single dataframe\n",
    "        df = pd.DataFrame(company_name, columns=['Company Name'])\n",
    "        df['Job Title'] = pd.DataFrame(job_title)\n",
    "        df['Location'] = pd.DataFrame(company_location)\n",
    "        df['Website URL'] = pd.DataFrame(website_url)\n",
    "\n",
    "        # Return Job Title if there are more than 1 matched word else return NaN\n",
    "        df['Job Title'] = df['Job Title'].apply(lambda x: linkedin_scraper.job_title_filter(x, job_title_input))\n",
    "\n",
    "        # Return Location if User Job Location in Scraped Location else return NaN\n",
    "        df['Location'] = df['Location'].apply(lambda x: x if job_location.lower() in x.lower() else np.nan)\n",
    "        \n",
    "        # Drop Null Values and Reset Index\n",
    "        df = df.dropna()\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        return df \n",
    "        \n",
    "\n",
    "    def scrap_job_description(driver, df, job_count):\n",
    "        \n",
    "        # Get URL into List\n",
    "        website_url = df['Website URL'].tolist()\n",
    "        \n",
    "        # Scrap the Job Description\n",
    "        job_description = []\n",
    "        description_count = 0\n",
    "\n",
    "        for i in range(0, len(website_url)):\n",
    "            try:\n",
    "                # Open the Link in LinkedIn\n",
    "                linkedin_scraper.open_link(driver, website_url[i])\n",
    "\n",
    "                # Click on Show More Button\n",
    "                driver.find_element(by=By.CSS_SELECTOR, value='button[data-tracking-control-name=\"public_jobs_show-more-html-btn\"]').click()\n",
    "                driver.implicitly_wait(5)\n",
    "                time.sleep(1)\n",
    "\n",
    "                # Get Job Description\n",
    "                description = driver.find_elements(by=By.CSS_SELECTOR, value='div[class=\"show-more-less-html__markup relative overflow-hidden\"]')\n",
    "                data = [i.text for i in description][0]\n",
    "                \n",
    "                # Check Description length and Duplicate\n",
    "                if len(data.strip()) > 0 and data not in job_description:\n",
    "                    job_description.append(data)\n",
    "                    description_count += 1\n",
    "                else:\n",
    "                    job_description.append('Description Not Available')\n",
    "            \n",
    "            # If any unexpected issue \n",
    "            except:\n",
    "                job_description.append('Description Not Available')\n",
    "            \n",
    "            # Check Description Count reach User Job Count\n",
    "            if description_count == job_count:\n",
    "                break\n",
    "\n",
    "        # Filter the Job Description\n",
    "        df = df.iloc[:len(job_description), :]\n",
    "\n",
    "        # Add Job Description in Dataframe\n",
    "        df['Job Description'] = pd.DataFrame(job_description, columns=['Description'])\n",
    "        df['Job Description'] = df['Job Description'].apply(lambda x: np.nan if x=='Description Not Available' else x)\n",
    "        df = df.dropna()\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "\n",
    "    def display_data_userinterface(df_final):\n",
    "\n",
    "        # Display the Data in User Interface\n",
    "        add_vertical_space(1)\n",
    "        if len(df_final) > 0:\n",
    "            for i in range(0, len(df_final)):\n",
    "                \n",
    "                st.markdown(f'<h3 style=\"color: orange;\">Job Posting Details : {i+1}</h3>', unsafe_allow_html=True)\n",
    "                st.write(f\"Company Name : {df_final.iloc[i,0]}\")\n",
    "                st.write(f\"Job Title    : {df_final.iloc[i,1]}\")\n",
    "                st.write(f\"Location     : {df_final.iloc[i,2]}\")\n",
    "                st.write(f\"Website URL  : {df_final.iloc[i,3]}\")\n",
    "\n",
    "                with st.expander(label='Job Desription'):\n",
    "                    st.write(df_final.iloc[i, 4])\n",
    "                add_vertical_space(3)\n",
    "        \n",
    "        else:\n",
    "            st.markdown(f'<h5 style=\"text-align: center;color: orange;\">No Matching Jobs Found</h5>', \n",
    "                                unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "    def main():\n",
    "        \n",
    "        # Initially set driver to None\n",
    "        driver = None\n",
    "        \n",
    "        try:\n",
    "            job_title_input, job_location, job_count, submit = linkedin_scraper.get_userinput()\n",
    "            add_vertical_space(2)\n",
    "            \n",
    "            if submit:\n",
    "                if job_title_input != [] and job_location != '':\n",
    "                    \n",
    "                    with st.spinner('Chrome Webdriver Setup Initializing...'):\n",
    "                        driver = linkedin_scraper.webdriver_setup()\n",
    "                                       \n",
    "                    with st.spinner('Loading More Job Listings...'):\n",
    "\n",
    "                        # build URL based on User Job Title Input\n",
    "                        link = linkedin_scraper.build_url(job_title_input, job_location)\n",
    "\n",
    "                        # Open the Link in LinkedIn and Scroll Down the Page\n",
    "                        linkedin_scraper.link_open_scrolldown(driver, link, job_count)\n",
    "\n",
    "                    with st.spinner('scraping Job Details...'):\n",
    "\n",
    "                        # Scraping the Company Name, Location, Job Title and URL Data\n",
    "                        df = linkedin_scraper.scrap_company_data(driver, job_title_input, job_location)\n",
    "\n",
    "                        # Scraping the Job Descriptin Data\n",
    "                        df_final = linkedin_scraper. scrap_job_description(driver, df, job_count)\n",
    "                    \n",
    "                    # Display the Data in User Interface\n",
    "                    linkedin_scraper.display_data_userinterface(df_final)\n",
    "\n",
    "                \n",
    "                # If User Click Submit Button and Job Title is Empty\n",
    "                elif job_title_input == []:\n",
    "                    st.markdown(f'<h5 style=\"text-align: center;color: orange;\">Job Title is Empty</h5>', \n",
    "                                unsafe_allow_html=True)\n",
    "                \n",
    "                elif job_location == '':\n",
    "                    st.markdown(f'<h5 style=\"text-align: center;color: orange;\">Job Location is Empty</h5>', \n",
    "                                unsafe_allow_html=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            add_vertical_space(2)\n",
    "            st.markdown(f'<h5 style=\"text-align: center;color: orange;\">{e}</h5>', unsafe_allow_html=True)\n",
    "        \n",
    "        finally:\n",
    "            if driver:\n",
    "                driver.quit()\n",
    "\n",
    "\n",
    "# Streamlit Configuration Setup\n",
    "streamlit_config()\n",
    "add_vertical_space(2)\n",
    "\n",
    "# Sidebar for navigation\n",
    "with st.sidebar:\n",
    "    add_vertical_space(4)\n",
    "    option = option_menu(menu_title='', options=['Summary', 'Strength', 'Weakness', 'Job Titles', 'Skills & Courses', 'Resume Tips', 'LinkedIn Jobs'],\n",
    "                         icons=['house-fill', 'database-fill', 'pass-fill', 'list-ul', 'tools', 'clipboard-check','linkedin'])\n",
    "\n",
    "    # User Upload the Resume\n",
    "    add_vertical_space(1)\n",
    "    pdf = st.file_uploader(label='Upload Your Resume', type='pdf')\n",
    "    add_vertical_space(1)\n",
    "\n",
    "    # Enter OpenAI API Key\n",
    "    col1, col2 = st.columns([0.6, 0.4])\n",
    "    with col1:\n",
    "        openai_api_key = st.text_input(label='Enter OpenAI API Key', type='password')\n",
    "    add_vertical_space(2)\n",
    "\n",
    "    # Click on Submit Button\n",
    "    submit = st.button(label='Submit')\n",
    "    add_vertical_space(1)\n",
    "    \n",
    "\n",
    "if pdf is not None and openai_api_key != '':\n",
    "        try:\n",
    "            with st.spinner('Processing...'):\n",
    "\n",
    "                # Process all sections after upload\n",
    "                if option == 'Summary':\n",
    "                    summary = resume_analyzer.resume_summary(pdf, openai_api_key)\n",
    "                    st.markdown(f'<h4 style=\"color: orange;\">Summary:</h4>', unsafe_allow_html=True)\n",
    "                    st.write(summary)\n",
    "\n",
    "                elif option == 'Strength':\n",
    "                    strength = resume_analyzer.resume_strength(pdf, openai_api_key)\n",
    "                    st.markdown(f'<h4 style=\"color: orange;\">Strength:</h4>', unsafe_allow_html=True)\n",
    "                    st.write(strength)\n",
    "\n",
    "                elif option == 'Weakness':\n",
    "                    weakness = resume_analyzer.resume_weakness(pdf, openai_api_key)\n",
    "                    st.markdown(f'<h4 style=\"color: orange;\">Weakness and Suggestions:</h4>', unsafe_allow_html=True)\n",
    "                    st.write(weakness)\n",
    "\n",
    "                elif option == 'Job Titles':\n",
    "                    job_titles = resume_analyzer.job_title_suggestion(pdf, openai_api_key)\n",
    "                    st.markdown(f'<h4 style=\"color: orange;\">Job Titles:</h4>', unsafe_allow_html=True)\n",
    "                    st.write(job_titles)\n",
    "\n",
    "                elif option == 'Skills & Courses':\n",
    "\n",
    "                    skills_result, recommended_skills_result, courses = resume_analyzer.skills_and_courses(pdf, openai_api_key)\n",
    "                    \n",
    "                    # Display skills you have\n",
    "                    st.markdown(f'<h4 style=\"color: orange;\">Skills You Have:</h4>', unsafe_allow_html=True)\n",
    "                    st.write(skills_result)\n",
    "\n",
    "                    # Display recommended skills\n",
    "                    st.markdown(f'<h4 style=\"color: orange;\">Recommended Skills:</h4>', unsafe_allow_html=True)\n",
    "                    st.write(recommended_skills_result)\n",
    "\n",
    "                    #Display courses recommended\n",
    "                    st.markdown(f'<h4 style=\"color: orange;\">Recommended Courses:</h4>', unsafe_allow_html=True)\n",
    "                    st.write(courses)\n",
    "                    \n",
    "                elif option == 'Resume Tips':\n",
    "                    tips = resume_analyzer.resume_tips(pdf, openai_api_key)\n",
    "                    st.markdown(f'<h4 style=\"color: orange;\">Resume Tips:</h4>', unsafe_allow_html=True)\n",
    "                    st.write(tips)\n",
    "\n",
    "                elif option == 'LinkedIn Jobs':\n",
    "                    linkedin_jobs = linkedin_scraper.main()\n",
    "                    st.markdown(f'<h4 style=\"color: orange;\">LinkedIn Jobs:</h4>', unsafe_allow_html=True)\n",
    "                    st.write(linkedin_jobs)\n",
    "                    \n",
    "                    \n",
    "        except Exception as e:\n",
    "            st.error(f\"Error: {str(e)}\")\n",
    "\n",
    "elif pdf is None:\n",
    "        st.markdown(f'<h5 style=\"text-align: center;color: orange;\">Please Upload Your Resume</h5>', unsafe_allow_html=True)\n",
    "\n",
    "elif openai_api_key == '':\n",
    "        st.markdown(f'<h5 style=\"text-align: center;color: orange;\">Please Enter OpenAI API Key</h5>', unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d72811-eae0-42ab-a66d-4a8ac01afbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8502\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://10.0.0.43:8502\u001b[0m\n",
      "\u001b[0m\n",
      "/Users/ramyasritellakula/Downloads/Resume_606/app.py:9: LangChainDeprecationWarning: Importing OpenAIEmbeddings from langchain.embeddings is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.embeddings import OpenAIEmbeddings\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.embeddings import OpenAIEmbeddings\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
      "  from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "/Users/ramyasritellakula/Downloads/Resume_606/app.py:10: LangChainDeprecationWarning: Importing FAISS from langchain.vectorstores is deprecated. Please replace deprecated imports:\n",
      "\n",
      ">> from langchain.vectorstores import FAISS\n",
      "\n",
      "with new imports of:\n",
      "\n",
      ">> from langchain_community.vectorstores import FAISS\n",
      "You can use the langchain cli to **automatically** upgrade many imports. Please see documentation here <https://python.langchain.com/docs/versions/v0_2/>\n",
      "  from langchain.vectorstores import FAISS\n",
      "/opt/anaconda3/lib/python3.11/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.chat_models import ChatOpenAI`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/langchain/chat_models/__init__.py:33: LangChainDeprecationWarning: Importing chat models from langchain is deprecated. Importing from langchain will no longer be supported as of langchain==0.2.0. Please import from langchain-community instead:\n",
      "\n",
      "`from langchain_community.chat_models import ChatOpenAI`.\n",
      "\n",
      "To install langchain-community run `pip install -U langchain-community`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc0f55-4af2-47bf-a6e8-39b25b6810d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
